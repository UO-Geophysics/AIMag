{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60e68d88",
   "metadata": {},
   "outputs": [],
   "source": [
    "### ----- Imports ----- ###\n",
    "\n",
    "import sys\n",
    "sys.path.append('/home/sdybing/neic-mlaapde')\n",
    "\n",
    "from mlaapde.access import MLAAPDE_Access\n",
    "from mlaapde import UTC\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os.path\n",
    "import os\n",
    "import shutil\n",
    "import glob\n",
    "import h5py\n",
    "import tensorflow as tf\n",
    "\n",
    "os.environ['TF_GPU_ALLOCATOR'] = 'cuda_malloc_async'\n",
    "os.environ['TF_FORCE_GPU_ALLOW_GROWTH'] = 'true'\n",
    "\n",
    "\n",
    "# mlpa = MLAAPDE_Access(data_dir = '/data/hank/mlaapde_subset/data', random_seed = 616) # 3 months\n",
    "# dataset = 'subset'\n",
    "\n",
    "mlpa = MLAAPDE_Access(data_dir = '/data/hank/mlaapde_v1b/data', random_seed = 616)\n",
    "dataset = 'v1b'\n",
    "\n",
    "mlpa.data_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e378f17c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import backend as K\n",
    "\n",
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "        logical_gpus = tf.config.list_logical_devices('GPU')\n",
    "        print(len(gpus), \"Physical GPU,\", len(logical_gpus), \"Logical GPUs\")\n",
    "    except RuntimeError as e:\n",
    "        print(e)\n",
    "        raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8b035e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#mlpa.default_args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b303e5eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "### ----- Parameters ----- ###\n",
    "\n",
    "# Where to save the products\n",
    "models_figs_path = '/home/sdybing/neic-mlaapde/allwaveforms/decimated/'\n",
    "\n",
    "# MLAAPDE/data generation params\n",
    "#nsamp = False # Samples of waveforms to load from MLAAPDE\n",
    "#n_train_samp = 1000000\n",
    "#n_valid_samp = 200000\n",
    "#nsamp = n_train_samp + n_valid_samp\n",
    "sr = 40 # Sampling rate\n",
    "trim_sec = 60 # Trimming amount around phase pick to get from MLAAPDE\n",
    "trim_pre_sec = trim_sec\n",
    "trim_post_sec = trim_pre_sec\n",
    "window_len = trim_pre_sec + trim_post_sec\n",
    "#train_split = 0.8 # Percentage of data used in training\n",
    "#valid_split = 0.2 # Percentage of data used for validation\n",
    "n_channels = 3 # Instrument channels\n",
    "cut_lens = [7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 25, 30, 35, 40, 50, 60, 70, 80, 90, 100, 110, 120]\n",
    "cut_lens_finish = [70, 80, 90, 100, 110, 120]\n",
    "test_cut_lens = [7, 8]\n",
    "desired_shift = 3\n",
    "max_shift = desired_shift * 2 # Since the shifting method actually makes it half what this value is set to\n",
    "min_snr_db = False\n",
    "max_snr_db = False\n",
    "log_progress_fraction = 100\n",
    "valid_phases = ['P', 'Pn', 'Pg']\n",
    "cast_dtype = np.float32\n",
    "\n",
    "# Training/model params\n",
    "epochs_number = 200\n",
    "batch_size = int(256) # Reducing to help memory\n",
    "monte_carlo_sampling = 50\n",
    "drop_rate = 0.5\n",
    "filters = [32, 64, 96, 128, 256] \n",
    "\n",
    "# Used if loading a trained model\n",
    "training_samps = 100000 \n",
    "training_dataset = 'v1b'\n",
    "shift_status = 'shifted'\n",
    "model_folder_path = '/home/sdybing/neic-mlaapde/allwaveforms/float32/'\n",
    "\n",
    "# To make end error plots\n",
    "mean_errors = []\n",
    "std_errors = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8034ef08",
   "metadata": {},
   "outputs": [],
   "source": [
    "### ----- Where are the HDF5 files getting saved? ----- ###\n",
    "\n",
    "# Location of HDF5 data files\n",
    "hdf5_save_dir = '/data/sdybing/allwaveforms/decimated/'\n",
    "if os.path.isdir(hdf5_save_dir):\n",
    "    pass\n",
    "else:\n",
    "    os.makedirs(hdf5_save_dir)\n",
    "\n",
    "# Pick extra labels and set keyword arguments for data parameters\n",
    "return_labels = ['source_magnitude', 'source_magnitude_type', 'snr_db', 'phase_id']\n",
    "kwargs = {'valid_phases':valid_phases, 'labels':return_labels, 'trim_pre_sec':trim_pre_sec, 'trim_post_sec':trim_post_sec, 'min_snr_db':min_snr_db, 'max_snr_db':max_snr_db, 'log_progress_fraction':log_progress_fraction, 'cast_dtype':cast_dtype}\n",
    "#kwargs = {'valid_phases':valid_phases, 'labels':return_labels, 'trim_pre_sec':trim_pre_sec, 'trim_post_sec':trim_post_sec, 'min_snr_db':min_snr_db, 'max_snr_db':max_snr_db, 'cast_dtype':cast_dtype}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "defec5b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "try_smaller_dataset = False\n",
    "\n",
    "if try_smaller_dataset:\n",
    "\n",
    "    hdf5_save_dir = '/data/sdybing/checksplitcode/'\n",
    "    n_train_samp = 40000\n",
    "    n_test_samp = 10000\n",
    "    nsamp = n_train_samp + n_test_samp\n",
    "\n",
    "    ### ----- Load the data from HDF5 files ----- ###\n",
    "\n",
    "    training_data = h5py.File(hdf5_save_dir + '/training_data.hdf5', 'r')\n",
    "\n",
    "    train_waves_t = training_data['waveforms'][:]\n",
    "    train_mags = training_data['magnitudes'][:]\n",
    "\n",
    "    validation_data = h5py.File(hdf5_save_dir + '/validation_data.hdf5', 'r')\n",
    "\n",
    "    valid_waves_t = validation_data['waveforms'][:]\n",
    "    valid_mags = validation_data['magnitudes'][:]\n",
    "\n",
    "    training_data.close()\n",
    "    validation_data.close()\n",
    "\n",
    "    print(train_waves_t.shape)\n",
    "    print(train_mags.shape)\n",
    "    print(valid_waves_t.shape)\n",
    "    print(valid_mags.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "657ff852",
   "metadata": {},
   "outputs": [],
   "source": [
    "run_before = True # If this code has been run before and the HDF5 files already exist, set this to True to save time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90324337",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "### ----- Load and save training data from MLAAPDE ----- ###\n",
    "\n",
    "if run_before == False:\n",
    "    \n",
    "    ### ----- Access the data from MLAAPDE ----- ###\n",
    "    \n",
    "    pt0 = UTC('2013-8-1') # Start of training window\n",
    "    pt1 = UTC('2018-10-1') # End of training window/start of validation window\n",
    "    pt2 = UTC('2020-1-1') # End of validation window\n",
    "    \n",
    "    print('Sampling catalog')\n",
    "    samps1, cat1 = mlpa.sample_catalog(time1 = pt0, time2 = pt1, nsamp = False, split = [1,0], **kwargs)\n",
    "    print('Sample_catalog done, splitting samples')\n",
    "    \n",
    "    samples = {}\n",
    "    samples['training'] = samps1['training']\n",
    "    print('Training samples split, getting waves')\n",
    "    \n",
    "    waves = {}\n",
    "    waves['training'] = mlpa.get_waves(samples['training'], **kwargs)\n",
    "    print('Get waves done, getting labels')\n",
    "    \n",
    "    labels = {}\n",
    "    labels['training'] = mlpa.get_labels(samples['training'], cat1, labels = return_labels)\n",
    "    print('Getting labels done, formatting arrays')\n",
    "\n",
    "    train_waves = waves['training']\n",
    "    print('Train waves done')\n",
    "    train_labels = labels['training']\n",
    "    print('Train labels done')\n",
    "    train_waves_t = train_waves.transpose(0,2,1)\n",
    "    print('Train waves transposed')\n",
    "    train_mags = train_labels['source_magnitude']\n",
    "    print('Train mags done')\n",
    "    train_mags_type = train_labels['source_magnitude_type']   \n",
    "    print('Train mag_types done')\n",
    "    train_snr_db = train_labels['snr_db']\n",
    "    print('Train SNRs done')\n",
    "    train_phase_id = train_labels['phase_id']\n",
    "    print('Train phase_ids done, next saving the HDF5')\n",
    "    \n",
    "    ### ----- Save the data to HDF5 file ----- ###\n",
    "            \n",
    "    with h5py.File(hdf5_save_dir + '/training_data.hdf5', 'w') as f1:\n",
    "        print('Saving waves')\n",
    "        waves = f1.create_dataset('waveforms', (train_waves_t.shape), data = train_waves_t, chunks = True)\n",
    "        print('Waves saved')\n",
    "        print('Saving mags')\n",
    "        mags = f1.create_dataset('magnitudes', (train_mags.shape), data = train_mags, chunks = True)\n",
    "        print('Mags saved')\n",
    "        print('Saving mag_types')\n",
    "        mags_type = f1.create_dataset('magnitude_types', (train_mags_type.shape), data = train_mags_type, chunks = True)\n",
    "        print('Mag_types saved')\n",
    "        print('Saving SNRs')\n",
    "        snr = f1.create_dataset('snr', (train_snr_db.shape), data = train_snr_db, chunks = True)\n",
    "        print('SNRs saved')\n",
    "        print('Saving phase_ids')\n",
    "        phase_id = f1.create_dataset('phase_id', (train_phase_id.shape), data = train_phase_id, chunks = True)\n",
    "        print('Phase_ids saved')\n",
    "\n",
    "#     f1 = h5py.File(hdf5_save_dir + '/training_data.hdf5', 'w')\n",
    "#     waves = f1.create_dataset('waveforms', (train_waves_t.shape), data = train_waves_t)\n",
    "#     mags = f1.create_dataset('magnitudes', (train_mags.shape), data = train_mags)\n",
    "#     mags_type = f1.create_dataset('magnitude_types', (train_mags_type.shape), data = train_mags_type)\n",
    "#     snr = f1.create_dataset('snr', (train_snr_db.shape), data = train_snr_db)\n",
    "#     phase_id = f1.create_dataset('phase_id', (train_phase_id.shape), data = train_phase_id)\n",
    "#     f1.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "052b28bb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "### ----- Load and save validation data from MLAAPDE ----- ###\n",
    "\n",
    "if run_before == False:\n",
    "    \n",
    "    ### ----- Access the data from MLAAPDE ----- ###\n",
    "    \n",
    "    pt0 = UTC('2013-8-1') # Start of training window\n",
    "    pt1 = UTC('2018-10-1') # End of training window/start of validation window\n",
    "    pt2 = UTC('2020-1-1') # End of validation window\n",
    "\n",
    "    samps2, cat2 = mlpa.sample_catalog(time1 = pt1, time2 = pt2, nsamp = False, split = [1,0], **kwargs)\n",
    "    \n",
    "    samples = {}\n",
    "    samples['validation'] = samps2['training']\n",
    "    \n",
    "    waves = {}\n",
    "    waves['validation'] = mlpa.get_waves(samples['validation'], **kwargs)\n",
    "    \n",
    "    labels = {}\n",
    "    labels['validation'] = mlpa.get_labels(samples['validation'], cat2, labels = return_labels)\n",
    "\n",
    "    valid_waves = waves['validation']\n",
    "    valid_labels = labels['validation']\n",
    "    valid_waves_t = valid_waves.transpose(0,2,1)\n",
    "    valid_mags = valid_labels['source_magnitude']\n",
    "    valid_mags_type = valid_labels['source_magnitude_type']\n",
    "    valid_snr_db = valid_labels['snr_db']\n",
    "    valid_phase_id = valid_labels['phase_id']\n",
    "    \n",
    "    ### ----- Save the data to HDF5 file ----- ###\n",
    "\n",
    "    f2 = h5py.File(hdf5_save_dir + '/validation_data.hdf5', 'w')\n",
    "    waves = f2.create_dataset('waveforms', (valid_waves_t.shape), data = valid_waves_t)\n",
    "    mags = f2.create_dataset('magnitudes', (valid_mags.shape), data = valid_mags)\n",
    "    mags_type = f2.create_dataset('magnitude_types', (valid_mags_type.shape), data = valid_mags_type)\n",
    "    snr = f2.create_dataset('snr', (valid_snr_db.shape), data = valid_snr_db)\n",
    "    phase_id = f2.create_dataset('phase_id', (valid_phase_id.shape), data = valid_phase_id)\n",
    "    f2.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "895208f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "### ----- Load the full dataset from HDF5 files ----- ###\n",
    "\n",
    "training_data = h5py.File(hdf5_save_dir + '/training_data.hdf5', 'r')\n",
    "dataset_names = list(training_data.keys())\n",
    "print(dataset_names)\n",
    "\n",
    "train_waves = training_data['waves'][:]\n",
    "train_mags = training_data['magnitude'][:]\n",
    "train_phase_id = training_data['phase_id'][:]\n",
    "\n",
    "validation_data = h5py.File(hdf5_save_dir + '/validation_data.hdf5', 'r')\n",
    "\n",
    "valid_waves = validation_data['waves'][:]\n",
    "valid_mags = validation_data['magnitude'][:]\n",
    "\n",
    "training_data.close()\n",
    "validation_data.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1708294e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_waves.shape)\n",
    "print(train_mags.shape)\n",
    "print(valid_waves.shape)\n",
    "print(valid_mags.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed679636",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_waves_t = train_waves.transpose(0,2,1)\n",
    "valid_waves_t = valid_waves.transpose(0,2,1)\n",
    "\n",
    "print(train_waves_t.shape)\n",
    "print(train_mags.shape)\n",
    "print(valid_waves_t.shape)\n",
    "print(valid_mags.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9038a885",
   "metadata": {},
   "outputs": [],
   "source": [
    "del train_waves\n",
    "del valid_waves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1828e5bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# isfinite = np.isfinite(train_waves_t)\n",
    "# #print(isfinite)\n",
    "# i = np.where(isfinite == False)[0]\n",
    "# print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d76dfbe2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# k = np.unique(i)\n",
    "# print(k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee02911e",
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = 741100\n",
    "idx2 = 741102\n",
    "# print(train_waves_t[idx])\n",
    "# print(train_waves_t[idx2])\n",
    "# plt.plot(train_waves_t[idx])\n",
    "# plt.show()\n",
    "# plt.plot(train_waves_t[idx2])\n",
    "# plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "330b38f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fixing the weird nan wave\n",
    "\n",
    "print(train_phase_id[idx])\n",
    "print(train_waves_t[idx])\n",
    "print(train_phase_id[idx2])\n",
    "print(train_waves_t[idx2])\n",
    "\n",
    "copy_wave = train_waves_t[0]\n",
    "copy_mag = train_mags[0]\n",
    "copy_wave2 = train_waves_t[1]\n",
    "copy_mag2 = train_mags[1]\n",
    "\n",
    "train_waves_t[idx] = copy_wave\n",
    "train_mags[idx] = copy_mag\n",
    "train_waves_t[idx2] = copy_wave2\n",
    "train_mags[idx2] = copy_mag2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "710c4d84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check to make sure it's good now\n",
    "\n",
    "print(train_waves_t[idx])\n",
    "print(train_mags[idx])\n",
    "print(train_mags[0])\n",
    "\n",
    "print(train_waves_t[idx2])\n",
    "print(train_mags[idx2])\n",
    "print(train_mags[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b82d944",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_train_samp = len(train_mags)\n",
    "n_valid_samp = len(valid_mags)\n",
    "nsamp = n_train_samp + n_valid_samp\n",
    "print(nsamp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f77f664",
   "metadata": {},
   "outputs": [],
   "source": [
    "##### -------- Functions -------- #####\n",
    "\n",
    "### ----- Predictions ----- ###\n",
    "\n",
    "class KerasDropoutPrediction(object):\n",
    "    def __init__(self, model):\n",
    "        self.model = model\n",
    "\n",
    "    def predict(self, x, n_iter = 10):\n",
    "        predM = []\n",
    "        auM = []\n",
    "\n",
    "        for itr in range(n_iter):\n",
    "\n",
    "            if itr == 0:\n",
    "                print('Making predictions...')\n",
    "            r = model.predict(x, batch_size = batch_size, verbose = 0)\n",
    "\n",
    "            pred = r[:, 0] \n",
    "            au = r[:, 1] \n",
    "            predM.append(pred.T)\n",
    "            auM.append(au.T)\n",
    "\n",
    "        predM = np.array(predM).reshape(n_iter, len(predM[0]))\n",
    "        auM = np.array(auM).reshape(n_iter, len(auM[0])) \n",
    "\n",
    "        yhat_mean = predM.mean(axis = 0)\n",
    "        yhat_squared_mean = np.square(predM).mean(axis = 0)\n",
    "\n",
    "        sigma_squared = 10**(auM)  # should be e, not 10?\n",
    "        sigma_squared_mean = sigma_squared.mean(axis = 0)\n",
    "\n",
    "        ep_unc = predM.std(axis = 0)  \n",
    "\n",
    "        combined = yhat_squared_mean - np.square(yhat_mean) + sigma_squared_mean\n",
    "\n",
    "        return yhat_mean, sigma_squared_mean, ep_unc, combined\n",
    "\n",
    "### ----- Training setup ----- ###\n",
    "\n",
    "def customLoss(yTrue, yPred):\n",
    "    y_hat = K.reshape(yPred[:, 0], [-1, 1]) \n",
    "    s = K.reshape(yPred[:, 1], [-1, 1])\n",
    "    return tf.reduce_sum(0.5 * K.exp(-1 * s) * K.square(K.abs(yTrue - y_hat)) + 0.5 * s, axis=1)\n",
    "\n",
    "### ----- Training callbacks ----- ###\n",
    "\n",
    "class PrintSomeValues(tf.keras.callbacks.Callback):\n",
    "    def on_epoch_begin(self, epoch, logs = {}):\n",
    "        print()\n",
    "        print(f'y_test[0:1] = {valid_mags[0:1]}.')\n",
    "        print(f'pred = {self.model.predict(shift_valid_waves_t[0:1])}.')\n",
    "\n",
    "### ----- Data generator ----- ###\n",
    "\n",
    "debug_generator = False\n",
    "debug_plot = False\n",
    "\n",
    "class dataGenerator(tf.keras.utils.Sequence):\n",
    "\n",
    "    def __init__(self, train_waves_t, train_mags, n_train_samp, window_len, cut_len, max_shift, sr, batch_size, n_channels, shuff = True, noise_rate = 0.5, flip_rate = 0.5, dropchan_rate = 0.05):\n",
    "    #def __init__(self, train_waves_t, train_mags, nsamp, window_len, cut_len, max_shift, sr, train_split, batch_size, n_channels, shuff = True, noise_rate = 0, flip_rate = 0, dropchan_rate = 0):\n",
    "        self.train_waves_t = train_waves_t\n",
    "        self.train_mags = train_mags\n",
    "        self.n_train_samp = n_train_samp\n",
    "        self.window_len = window_len\n",
    "        self.cut_len = cut_len\n",
    "        self.max_shift = max_shift\n",
    "        self.shift_len = self.cut_len - self.max_shift\n",
    "        self.sr = sr\n",
    "        self.shuff = shuff\n",
    "        self.batch_size = batch_size\n",
    "        self.full_lengthpts = int(self.window_len * self.sr)\n",
    "        self.cut_lengthpts = int(self.cut_len * self.sr)\n",
    "        self.shift_lengthpts = int(self.shift_len * self.sr)\n",
    "        self.lentraindata = int(self.n_train_samp)\n",
    "        self.middle = int(self.full_lengthpts / 2)\n",
    "        self.n_channels = n_channels\n",
    "        self.on_epoch_end()\n",
    "        self.noise_rate = noise_rate\n",
    "        self.flip_rate = flip_rate\n",
    "        self.dropchan_rate = dropchan_rate\n",
    "\n",
    "    def on_epoch_end(self): # Modify dataset between epochs\n",
    "        self.indexes = np.arange(self.lentraindata, dtype = int) # Array of integers for the training data length\n",
    "        if self.shuff == True:\n",
    "            np.random.shuffle(self.indexes) # Shuffle those indices if indicated\n",
    "\n",
    "    def __len__(self) : # Number of batches in the sequences\n",
    "        return int(self.lentraindata / self.batch_size) # Length of training data divided by the chosen batch size\n",
    "\n",
    "    def __data_generation(self, indexes):\n",
    "        \n",
    "        # Initialization\n",
    "        if debug_generator: print('Initial empty shapes!')\n",
    "        y = np.ones((self.batch_size,))\n",
    "        \n",
    "        full_x = np.zeros((self.batch_size, self.full_lengthpts, self.n_channels)) # Shape is batch size by number of samples (window * sps) by number of channels\n",
    "        if debug_generator: print('Full X shape:' + str(full_x.shape))\n",
    "        \n",
    "        cut_x = np.zeros((self.batch_size, self.cut_lengthpts, self.n_channels)) # Shape is batch size by number of samples (window * sps) by number of channels\n",
    "        if debug_generator: print('Cut X shape:' + str(cut_x.shape))\n",
    "        \n",
    "        shift_x = np.zeros((self.batch_size, self.shift_lengthpts, self.n_channels)) # Shape is batch size by number of samples (window * sps) by number of channels\n",
    "        if debug_generator: print('Shift X shape:' + str(shift_x.shape))\n",
    "            \n",
    "        x = shift_x.copy()\n",
    "    \n",
    "        # Make the augmentations\n",
    "        for i, ix in enumerate(indexes):\n",
    "            n = 0 # Counter to prevent dropping 3 channels\n",
    "            \n",
    "            if debug_generator: print('Augmenting!')\n",
    "            if debug_plot:\n",
    "                wvf_idx = np.random.choice(np.arange(0,len(self.train_mags),1))\n",
    "                #wvf_idx = 507\n",
    "                if i == wvf_idx:\n",
    "                    print('Waveform index: ' + str(wvf_idx))\n",
    "                    def plot_features(axis):\n",
    "                        axis.legend(loc = 'upper left', fontsize = 14)\n",
    "                        axis.set_xlim(0,self.shift_len)\n",
    "                        axis.set_ylim(-1.2,1.2)\n",
    "                        axis.axvline(self.shift_len/2, color = 'black', linestyle = '--', alpha = 0.7)\n",
    "                        axis.tick_params(axis = 'x', bottom = False, labelbottom = False)\n",
    "                    f, ((a0, a1, a2), (a3, a4, a5), (a6, a7, a8), (a9, a10, a11), (a12, a13, a14), (a15, a16, a17), (a18, a19, a20)) = plt.subplots(nrows = 7, ncols = 3, gridspec_kw={'height_ratios': [1, 1, 1, 0.75, 1, 1, 1]}, figsize = (22,10), dpi=300, facecolor = 'white')\n",
    "\n",
    "            # Original waveforms\n",
    "            if debug_generator: print(ix)\n",
    "            full_x[i,] = self.train_waves_t[ix,:,0:3]\n",
    "            y[i,] = self.train_mags[ix,]\n",
    "            if debug_generator: print('Original full lengthpts: ' + str(self.full_lengthpts))\n",
    "            if debug_generator: print('Original full x shape: ' + str(full_x.shape))\n",
    "            if debug_plot:\n",
    "                if i == wvf_idx:\n",
    "                    times = np.arange(0, self.window_len, 1/self.sr)\n",
    "                    \n",
    "                    a0.set_title('Original waveforms', fontsize = 16)\n",
    "                    a0.plot(times, full_x[i,:,0], color = 'C0', label = 'E') \n",
    "                    a0.legend(loc = 'upper left', fontsize = 14)\n",
    "                    a0.set_xlim(0,self.window_len)\n",
    "                    a0.set_ylim(-1.2,1.2)\n",
    "                    a0.axvline(self.window_len/2, color = 'black', linestyle = '--', alpha = 0.7)\n",
    "                    a0.tick_params(axis = 'x', bottom = False, labelbottom = False)\n",
    "\n",
    "                    a3.plot(times, full_x[i,:,1], color = 'C1', label = 'N')\n",
    "                    a3.set_ylabel('Stream-normalized amplitude', fontsize = 14)\n",
    "                    a3.legend(loc = 'upper left', fontsize = 14)\n",
    "                    a3.set_xlim(0,self.window_len)\n",
    "                    a3.set_ylim(-1.2,1.2)\n",
    "                    a3.axvline(self.window_len/2, color = 'black', linestyle = '--', alpha = 0.7)\n",
    "                    a3.tick_params(axis = 'x', bottom = False, labelbottom = False)\n",
    "\n",
    "                    a6.plot(times, full_x[i,:,2], color = 'C2', label = 'Z')\n",
    "                    a6.set_xlabel('Time (s)', fontsize = 14)\n",
    "                    a6.legend(loc = 'upper left', fontsize = 14)\n",
    "                    a6.set_xlim(0,self.window_len)\n",
    "                    a6.set_ylim(-1.2,1.2)\n",
    "                    a6.axvline(self.window_len/2, color = 'black', linestyle = '--', alpha = 0.7)\n",
    "                    a6.tick_params(axis = 'x', bottom = True, labelbottom = True)\n",
    "\n",
    "            # Cut to the window length \n",
    "            cut_x[i,] = full_x[i, int(self.middle - (self.cut_len/2)*self.sr) : int(self.middle + (self.cut_len/2)*self.sr), 0:3]\n",
    "            if debug_generator: print('Cut lengthpts: ' + str(self.cut_lengthpts))\n",
    "            if debug_generator: print('Cut x shape: ' + str(cut_x.shape))\n",
    "            if debug_plot:\n",
    "                if i == wvf_idx:\n",
    "                    cut_times = np.arange(0, self.cut_len, 1/self.sr)\n",
    "\n",
    "                    a1.set_title('Trimming to desired window length', fontsize = 16)\n",
    "                    a1.plot(cut_times, cut_x[i,:,0], color = 'C0', label = 'E')\n",
    "                    a1.legend(loc = 'upper left', fontsize = 14)\n",
    "                    a1.set_xlim(0,self.cut_len)\n",
    "                    a1.set_ylim(-1.2,1.2)\n",
    "                    a1.axvline(self.cut_len/2, color = 'black', linestyle = '--', alpha = 0.7)\n",
    "                    a1.tick_params(axis = 'x', bottom = False, labelbottom = False)\n",
    "\n",
    "                    a4.plot(cut_times, cut_x[i,:,1], color = 'C1', label = 'N')\n",
    "                    a4.set_ylabel('Stream-normalized amplitude', fontsize = 14)\n",
    "                    a4.legend(loc = 'upper left', fontsize = 14)\n",
    "                    a4.set_xlim(0,self.cut_len)\n",
    "                    a4.set_ylim(-1.2,1.2)\n",
    "                    a4.axvline(self.cut_len/2, color = 'black', linestyle = '--', alpha = 0.7)\n",
    "                    a4.tick_params(axis = 'x', bottom = False, labelbottom = False)\n",
    "\n",
    "                    a7.plot(cut_times, cut_x[i,:,2], color = 'C2', label = 'Z')\n",
    "                    a7.set_xlabel('Time (s)', fontsize = 14)\n",
    "                    a7.legend(loc = 'upper left', fontsize = 14)\n",
    "                    a7.set_xlim(0,self.cut_len)\n",
    "                    a7.set_ylim(-1.2,1.2)\n",
    "                    a7.axvline(self.cut_len/2, color = 'black', linestyle = '--', alpha = 0.7)\n",
    "                    a7.tick_params(axis = 'x', bottom = True, labelbottom = True)\n",
    "            \n",
    "            # Shifting up to 3 seconds\n",
    "            self.time_offset = np.random.uniform(low = 0, high = self.max_shift) # seconds\n",
    "            self.samps_offset = int(self.time_offset * self.sr)\n",
    "            self.start = self.samps_offset\n",
    "            self.end = int(self.start + self.shift_len * self.sr)\n",
    "            shift_x[i,] = cut_x[i, self.start : self.end, 0:3]\n",
    "            if debug_generator: print('Shift lengthpts: ' + str(self.shift_lengthpts))\n",
    "            if debug_generator: print('Shift x shape: '  + str(shift_x.shape))\n",
    "            if debug_plot:\n",
    "                if i == wvf_idx:\n",
    "                    print(self.time_offset)\n",
    "                    print(self.shift_len)\n",
    "                    shift_times = np.arange(0, self.shift_len, 1/self.sr)\n",
    "\n",
    "                    a2.set_title('Shifted ' + str(round(self.time_offset,1)) + ' seconds', fontsize = 16)\n",
    "                    a2.plot(shift_times, shift_x[i,:,0], color = 'C0', label = 'E')\n",
    "                    plot_features(a2)\n",
    "\n",
    "                    a5.plot(shift_times, shift_x[i,:,1], color = 'C1', label = 'N')\n",
    "                    a5.set_ylabel('Stream-normalized amplitude', fontsize = 14)\n",
    "                    plot_features(a5)\n",
    "\n",
    "                    a8.plot(shift_times, shift_x[i,:,2], color = 'C2', label = 'Z')\n",
    "                    a8.set_xlabel('Time (s)', fontsize = 14)\n",
    "                    plot_features(a8)\n",
    "                    a8.tick_params(axis = 'x', bottom = True, labelbottom = True)\n",
    "            \n",
    "            x[i,] = shift_x[i,]\n",
    "            if debug_generator: print('Renamed to x shape: ' + str(x.shape))\n",
    "            \n",
    "            # Add extra noise\n",
    "            if(np.random.random() < self.noise_rate):\n",
    "                x[i,:,0] = x[i,:,0] + np.random.normal(0, np.random.uniform(0.01, 0.15), self.shift_lengthpts)\n",
    "                x[i,:,1] = x[i,:,1] + np.random.normal(0, np.random.uniform(0.01, 0.15), self.shift_lengthpts)\n",
    "                x[i,:,2] = x[i,:,2] + np.random.normal(0, np.random.uniform(0.01, 0.15), self.shift_lengthpts)\n",
    "            if debug_plot:\n",
    "                if i == wvf_idx:\n",
    "                    a12.set_title('Extra noise', fontsize = 16)\n",
    "                    a12.plot(shift_times, x[i,:,0], color = 'C0', label = 'E') \n",
    "                    plot_features(a12)\n",
    "\n",
    "                    a15.plot(shift_times, x[i,:,1], color = 'C1', label = 'N')\n",
    "                    a15.set_ylabel('Stream-normalized amplitude', fontsize = 14)\n",
    "                    plot_features(a15)\n",
    "\n",
    "                    a18.plot(shift_times, x[i,:,2], color = 'C2', label = 'Z')\n",
    "                    a18.set_xlabel('Time (s)', fontsize = 14)\n",
    "                    plot_features(a18)\n",
    "                    a18.tick_params(axis = 'x', bottom = True, labelbottom = True)\n",
    "            x[i,] = x[i,] / np.max(np.abs(x[i,])) # normalizing again now that it's cut\n",
    "\n",
    "            # Flip horizontal channels\n",
    "            if(np.random.random() < self.flip_rate):\n",
    "                flip =  x[i,:,0].copy()\n",
    "                x[i,:,0] =  x[i,:,1]\n",
    "                x[i,:,1] =  flip\n",
    "            if debug_plot:\n",
    "                if i == wvf_idx:\n",
    "                    a13.set_title('Flip horizontal components', fontsize = 16)\n",
    "                    a13.plot(shift_times, x[i,:,0], color = 'C0', label = 'E') \n",
    "                    plot_features(a13)\n",
    "\n",
    "                    a16.plot(shift_times, x[i,:,1], color = 'C1', label = 'N')\n",
    "                    a16.set_ylabel('Stream-normalized amplitude', fontsize = 14)\n",
    "                    plot_features(a16)\n",
    "\n",
    "                    a19.plot(shift_times, x[i,:,2], color = 'C2', label = 'Z')\n",
    "                    a19.set_xlabel('Time (s)', fontsize = 14)\n",
    "                    plot_features(a19)\n",
    "                    a19.tick_params(axis = 'x', bottom = True, labelbottom = True)\n",
    "            \n",
    "            # Drop channels\n",
    "            if(np.random.random() < self.dropchan_rate):\n",
    "                x[i,:,0] = 0\n",
    "                n += 1\n",
    "            if(np.random.random() < self.dropchan_rate):\n",
    "                x[i,:,1] = 0\n",
    "                n += 1\n",
    "            if(np.random.random() < self.dropchan_rate):\n",
    "                if n == 2:\n",
    "                    pass\n",
    "                else:\n",
    "                    x[i,:,2] = 0\n",
    "            if debug_plot:\n",
    "                if i == wvf_idx:\n",
    "                    a14.set_title('Drop channel', fontsize = 16)\n",
    "                    a14.plot(shift_times, x[i,:,0], color = 'C0', label = 'E') \n",
    "                    plot_features(a14)\n",
    "\n",
    "                    a17.plot(shift_times, x[i,:,1], color = 'C1', label = 'N')\n",
    "                    a17.set_ylabel('Stream-normalized amplitude', fontsize = 14)\n",
    "                    plot_features(a17)\n",
    "\n",
    "                    a20.plot(shift_times, x[i,:,2], color = 'C2', label = 'Z')\n",
    "                    a20.set_xlabel('Time (s)', fontsize = 14)\n",
    "                    plot_features(a20)\n",
    "                    a20.tick_params(axis = 'x', bottom = True, labelbottom = True)\n",
    "            \n",
    "            if debug_plot:\n",
    "                if i == wvf_idx:\n",
    "                    a9.set_visible(False)\n",
    "                    a10.set_visible(False)\n",
    "                    a11.set_visible(False)\n",
    "\n",
    "                    plt.subplots_adjust(hspace = 0)\n",
    "                    plt.show()\n",
    "                    plt.close();\n",
    "            \n",
    "            if debug_generator: print('Final x shape: ' + str(x.shape))\n",
    "\n",
    "        return x, y\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        iii = self.indexes[index * self.batch_size : (index + 1) * self.batch_size]\n",
    "        x, y = self.__data_generation(iii)\n",
    "\n",
    "        return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23f9a5d8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "########## STUFF THAT NEEDS LOOPING ##########\n",
    "\n",
    "debug = False\n",
    "\n",
    "for cut_len in cut_lens_finish:\n",
    "    print('Cut len: ' + str(cut_len))\n",
    "    \n",
    "    ### ----- Where are the trained models/figures getting saved? ----- ###\n",
    "\n",
    "    save_dir = models_figs_path + str(dataset) + '_' + str(nsamp) + 'samps_' + str(cut_len-6) + 's_window'\n",
    "    if os.path.isdir(save_dir):\n",
    "        pass\n",
    "    else: # deletes directory to start over: shutil.rmtree(save_dir)  \n",
    "        os.makedirs(save_dir)\n",
    "\n",
    "    ### ----- Cut and shift validation data to match the training data ----- ###\n",
    "\n",
    "    ## Cut ##\n",
    "    if debug:\n",
    "        rand = np.random.choice(np.arange(0,len(valid_mags),1))\n",
    "        print('Rand: ' + str(rand))\n",
    "        valid_times = np.arange(0, window_len, 1/sr)\n",
    "        plt.figure(facecolor = 'white')\n",
    "        plt.suptitle('Original validation data')\n",
    "        plt.subplot(311)\n",
    "        plt.plot(valid_times, valid_waves_t[rand,:,0], color = 'C0')\n",
    "        plt.subplot(312)\n",
    "        plt.plot(valid_times, valid_waves_t[rand,:,1], color = 'C1')\n",
    "        plt.subplot(313)\n",
    "        plt.plot(valid_times, valid_waves_t[rand,:,2], color = 'C2')\n",
    "        plt.subplots_adjust(hspace = 0)\n",
    "        plt.show();\n",
    "    \n",
    "    middle = int(valid_waves_t.shape[1] / 2)\n",
    "    if debug: print('Middle: ' + str(middle))\n",
    "    valid_size = int(n_valid_samp)\n",
    "    if debug: print('Valid size: ' + str(valid_size))\n",
    "    cut_valid_waves_t = np.zeros((valid_size, int(cut_len*sr), 3)) \n",
    "    if debug: print('Cut waves t shape: ' + str(cut_valid_waves_t.shape))\n",
    "\n",
    "    for i in range(len(valid_waves_t)):\n",
    "        cut_valid_waves_t[i,] = valid_waves_t[i, int(middle - (cut_len/2)*sr) : int(middle + (cut_len/2)*sr), 0:3]\n",
    "    if debug: print('Cut waves t shape: ' + str(cut_valid_waves_t.shape))\n",
    "    if debug:\n",
    "        valid_cut_times = np.arange(0, cut_len, 1/sr)\n",
    "        print('Rand: ' + str(rand))\n",
    "        plt.figure(facecolor = 'white')\n",
    "        plt.suptitle('Cut validation data')\n",
    "        plt.subplot(311)\n",
    "        plt.plot(valid_cut_times, cut_valid_waves_t[rand,:,0], color = 'C0')\n",
    "        plt.subplot(312)\n",
    "        plt.plot(valid_cut_times, cut_valid_waves_t[rand,:,1], color = 'C1')\n",
    "        plt.subplot(313)\n",
    "        plt.plot(valid_cut_times, cut_valid_waves_t[rand,:,2], color = 'C2')\n",
    "        plt.subplots_adjust(hspace = 0)\n",
    "        plt.show();\n",
    "\n",
    "    ## Shift ##\n",
    "    shift_len = cut_len - max_shift\n",
    "    if debug: print('Shift len: ' + str(shift_len))\n",
    "    time_offset = np.random.uniform(low = 0, high = max_shift, size = valid_size)\n",
    "    shift_valid_waves_t = np.zeros((valid_size, int(shift_len * sr), 3)) \n",
    "\n",
    "    for ii, offset in enumerate(time_offset):\n",
    "        bin_offset = int(offset * sr)\n",
    "        start_bin = bin_offset \n",
    "        end_bin = int(start_bin + shift_len * sr)\n",
    "        shift_valid_waves_t[ii, :, 0] = cut_valid_waves_t[ii, start_bin:end_bin, 0] \n",
    "        shift_valid_waves_t[ii, :, 1] = cut_valid_waves_t[ii, start_bin:end_bin, 1]\n",
    "        shift_valid_waves_t[ii, :, 2] = cut_valid_waves_t[ii, start_bin:end_bin, 2]\n",
    "\n",
    "    if debug: print('Shift waves t shape: ' + str(shift_valid_waves_t.shape))\n",
    "    if debug:\n",
    "        valid_shift_times = np.arange(0, shift_len, 1/sr)\n",
    "        print('Rand: ' + str(rand))\n",
    "        plt.figure(facecolor = 'white')\n",
    "        plt.suptitle('Shifted validation data')\n",
    "        plt.subplot(311)\n",
    "        plt.plot(valid_shift_times, shift_valid_waves_t[rand,:,0], color = 'C0')\n",
    "        plt.subplot(312)\n",
    "        plt.plot(valid_shift_times, shift_valid_waves_t[rand,:,1], color = 'C1')\n",
    "        plt.subplot(313)\n",
    "        plt.plot(valid_shift_times, shift_valid_waves_t[rand,:,2], color = 'C2')\n",
    "        plt.subplots_adjust(hspace = 0)\n",
    "        plt.show();\n",
    "\n",
    "    ### ----- Initialize the model and training setup ----- ###\n",
    "    \n",
    "    inp1 = tf.keras.layers.Input(shape = ((cut_len - max_shift)*sr, n_channels), name = 'input_layer') \n",
    "    e = tf.keras.layers.Conv1D(filters[1], 3, padding = 'same')(inp1) \n",
    "    e = tf.keras.layers.Dropout(drop_rate)(e, training = True)\n",
    "    e = tf.keras.layers.MaxPooling1D(4, padding = 'same')(e)\n",
    "    e = tf.keras.layers.Conv1D(filters[0], 3, padding = 'same')(e) \n",
    "    e = tf.keras.layers.Dropout(drop_rate)(e, training = True)\n",
    "    e = tf.keras.layers.MaxPooling1D(4, padding = 'same')(e)\n",
    "    e = tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(100, return_sequences = False, dropout = 0.0, recurrent_dropout = 0.0))(e)\n",
    "    #e = tf.keras.layers.Dense(2)(e)\n",
    "    e = tf.keras.layers.Dense(1)(e)\n",
    "    o = tf.keras.layers.Activation('linear', name = 'output_layer')(e)\n",
    "    model = tf.keras.models.Model(inputs = [inp1], outputs = o)\n",
    "    #model.summary()\n",
    "\n",
    "    #model.compile(optimizer = 'Adam', loss = customLoss)\n",
    "    model.compile(optimizer = 'Adam', loss = tf.keras.losses.MeanSquaredError())\n",
    "    \n",
    "    model_name = str(dataset) + '_' + str(nsamp) + 'samps_' + str(shift_len) + 's'\n",
    "    lr_reducer = tf.keras.callbacks.ReduceLROnPlateau(factor = np.sqrt(0.1), cooldown = 0, patience = 4, min_lr = 0.5e-6)\n",
    "    m_name = str(model_name) + '_{epoch:03d}.h5' \n",
    "    filepath = os.path.join(save_dir, m_name)\n",
    "    early_stopping_monitor = tf.keras.callbacks.EarlyStopping(monitor = 'val_loss', patience = 5)\n",
    "    checkpoint = tf.keras.callbacks.ModelCheckpoint(filepath = filepath, monitor = 'val_loss', mode = 'auto', verbose = 1, save_best_only = True)\n",
    "    psv = PrintSomeValues()\n",
    "    callbacks = [lr_reducer, early_stopping_monitor, checkpoint, psv]\n",
    "    training_generator = dataGenerator(train_waves_t, train_mags, n_train_samp, window_len, cut_len, max_shift, sr, batch_size, n_channels)\n",
    "\n",
    "    ### ----- Train ----- ###\n",
    "\n",
    "    history = model.fit(training_generator, epochs = epochs_number, validation_data = (shift_valid_waves_t, valid_mags), callbacks = callbacks);\n",
    "\n",
    "    ### ----- Plot training curves ----- ###\n",
    "\n",
    "    plt.figure(facecolor = 'white')\n",
    "    plt.plot(history.history['loss'],label='Training Loss')\n",
    "    plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "    plt.legend()\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    #plt.show()\n",
    "    plt.savefig(save_dir + '/loss_curves_' + m_name + '.png')\n",
    "    plt.close();\n",
    "\n",
    "    ### ----- Make the predictions ----- ###\n",
    "\n",
    "    #kdp = KerasDropoutPrediction(model)\n",
    "    #predict, al_unc, ep_unc, comb = kdp.predict(shift_valid_waves_t, monte_carlo_sampling)\n",
    "    predict = model.predict(shift_valid_waves_t)\n",
    "\n",
    "    ### ----- Quick plot of the predictions vs. true magnitudes ----- ###\n",
    "\n",
    "    fig4, ax = plt.subplots(facecolor = 'white')\n",
    "    ax.scatter(valid_mags, predict, alpha = 0.4, facecolors = 'r', edgecolors = 'r')\n",
    "    ax.plot([valid_mags.min(), valid_mags.max()], [valid_mags.min(), valid_mags.max()], 'k--', alpha=1, lw=2)\n",
    "    ax.set_xlabel('Measured magnitude')\n",
    "    ax.set_ylabel('Predicted magnitude')\n",
    "    #plt.show()\n",
    "    fig4.savefig(save_dir + '/scatter_' + m_name + '.png')\n",
    "    plt.close();\n",
    "\n",
    "    ### ----- Rename things ----- ###\n",
    "\n",
    "    measured_mags = valid_mags\n",
    "    predicted_mags = predict.flatten()\n",
    "\n",
    "    ### ----- Calculate the error and standard deviation ----- ###\n",
    "\n",
    "    errors = []\n",
    "\n",
    "    for idx in range(len(predicted_mags)):\n",
    "        predicted = predicted_mags[idx]\n",
    "        measured = measured_mags[idx]\n",
    "        error = predicted - measured\n",
    "        errors.append(error)\n",
    "\n",
    "    mean_error = np.mean(np.array(errors))\n",
    "    std_error = np.std(np.array(errors))\n",
    "\n",
    "    print('Mean error: ' + str(round(mean_error,3)))\n",
    "    print('Error standard deviation: ' + str(round(std_error,2)))\n",
    "\n",
    "    mean_errors.append(mean_error)\n",
    "    std_errors.append(std_error)\n",
    "\n",
    "    ### ----- Make the box and whisker plots with STF magnitude line ----- ###\n",
    "\n",
    "    Tt = shift_len / 2\n",
    "    M0_dyncm = Tt**3 * (0.625 * 10**23)\n",
    "    Mw = ((2/3) * np.log10(M0_dyncm)) - 10.73 # M0 in dyne-cm\n",
    "\n",
    "    print('Rupture duration: ' + str(Tt) + ' seconds')\n",
    "    print('M0: ' + str(M0_dyncm) + ' dyne-cm')\n",
    "    print('Mw: ' + str(round(Mw,2)))\n",
    "\n",
    "    bins = np.arange(11,85,1)/10\n",
    "    data_bins = []\n",
    "\n",
    "    for abin in bins:\n",
    "        i = np.where(valid_mags == abin)[0]\n",
    "        predict_bin = np.array(predicted_mags[i])\n",
    "        data_bins.append(predict_bin)\n",
    "\n",
    "    fig = plt.figure(figsize =(14, 9), dpi = 300, facecolor = 'white')\n",
    "\n",
    "    fig.suptitle('MLAAPDE ' + str(dataset) + ' agumented dataset, tested with ' + str(int(n_valid_samp)) + ' ' + str(shift_len) + 's window samples shifted up to 3s', fontsize = 18, y = 0.96, color = 'black')\n",
    "    ax = fig.add_subplot(111)\n",
    "    ax.set_facecolor('white')\n",
    "    ax.text(x = 30, y = 8.8, s = 'Model: ' + m_name, fontsize = 13, color = 'black')\n",
    "    ax.grid(which = 'major', axis = 'y')\n",
    "    ax.grid(which = 'major', axis = 'x', markevery = [10,20,30,40,50])\n",
    "    ax.set_ylim(1,8.6)\n",
    "\n",
    "    bp = ax.boxplot(data_bins, notch = False, patch_artist = True)\n",
    "    ax.axvline((Mw-1)*10, color = 'green', linestyle = '--', linewidth = 2) # Position = (magnitude - 1)*10\n",
    "\n",
    "    for patch in bp['boxes']:\n",
    "        patch.set_facecolor('lightblue')\n",
    "        patch.set_edgecolor('blue')\n",
    "    for median in bp['medians']:\n",
    "        median.set(color ='blue', linewidth = 3)\n",
    "    for whisker in bp['whiskers']:\n",
    "        whisker.set(color ='blue', linewidth = 1)\n",
    "    for cap in bp['caps']:\n",
    "        cap.set(color ='blue', linewidth = 1)\n",
    "    for flier in bp['fliers']:\n",
    "        flier.set(marker ='+', color ='blue', alpha = 0.5)\n",
    "\n",
    "    bins_list = bins.tolist()\n",
    "    ax.set_xticklabels(bins_list, fontsize = 14, color = 'black')\n",
    "    ax.set_yticklabels([1, 2, 3, 4, 5, 6, 7, 8], fontsize = 14, color = 'black')\n",
    "    ax.set_ylabel('Predicted magnitude', fontsize = 16, color = 'black')\n",
    "    ax.set_xlabel('Measured magnitude', fontsize = 16, color = 'black')\n",
    "    ax.xaxis.set_major_locator(plt.MaxNLocator(8))\n",
    "    ax.plot((1.1,70),(1.1,8),'r--', linewidth = 3, alpha = 0.5)\n",
    "    ax.text(s = 'Testing results', x = 2, y = 8, fontsize = 18, backgroundcolor = 'lightskyblue', color = 'black')\n",
    "    ax.text(s = 'STF magnitude: ' + str(round(Mw,2)), x = 2, y = 7.5, fontsize = 18, backgroundcolor = 'lightgreen', color = 'black')\n",
    "\n",
    "    #plt.show()\n",
    "    plt.savefig(save_dir + '/boxplot_durline_' + m_name + '.png', format = 'PNG', facecolor = 'white', transparent = False)\n",
    "    plt.close();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "def61958",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savetxt(models_figs_path + str(dataset) + '_' + str(nsamp) + 'testsamp_' + str(training_samps) + 'trainsamp_meanerrors.txt', np.array(mean_errors))\n",
    "np.savetxt(models_figs_path + str(dataset) + '_' + str(nsamp) + 'testsamp_' + str(training_samps) + 'trainsamp_stderrors.txt', np.array(std_errors))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb192f63",
   "metadata": {},
   "outputs": [],
   "source": [
    "### ----- Plot error and std for all windows ----- ###\n",
    "\n",
    "shift_lengths = []\n",
    "\n",
    "for cut_len in cut_lens:\n",
    "    shift_len = cut_len - max_shift\n",
    "    shift_lengths.append(shift_len)\n",
    "    \n",
    "plt.figure(figsize = (10, 6), facecolor = 'white')\n",
    "plt.title('Testing errors/stds: models trained with\\n100,000 augmented samples shifted up to 3 seconds', fontsize = 16)\n",
    "plt.errorbar(shift_lengths, mean_errors, yerr = std_errors, fmt = '.', markersize = 10, ecolor = 'C1', capsize = 3, label = 'Error bars show 1 standard\\ndeviation above each point and\\n1 standard deviation below')\n",
    "plt.scatter(shift_lengths, mean_errors, color = 'C0')\n",
    "plt.grid()\n",
    "plt.xlabel('Window length (s)', fontsize = 14)\n",
    "plt.ylabel('Mean error\\n(predicted - measured magnitude)', fontsize = 14)\n",
    "plt.xticks(fontsize = 13)\n",
    "plt.yticks(fontsize = 13)\n",
    "plt.legend(fontsize = 12)\n",
    "plt.axhline(0, color = 'black', linestyle = '--', alpha = 0.75)\n",
    "\n",
    "#plt.show()\n",
    "plt.savefig(models_figs_path + str(dataset) + '_' + str(nsamp) + 'testsamp_' + str(training_samps) + 'trainsamp_all_errors_stds.png', format = 'PNG', facecolor = 'white', transparent = False)\n",
    "plt.close();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8265a6e5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc767f47",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5813a06e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2f7a653",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
